{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35bde6ba",
   "metadata": {},
   "source": [
    "# Food-101 â€” ViT Training Notebook\n",
    "\n",
    "This notebook trains a ViT model on Food-101 using Hugging Face `datasets` + `transformers`.\n",
    "Cells:\n",
    "1. Install dependencies\n",
    "2. Imports & environment checks\n",
    "3. Load & inspect dataset\n",
    "4. Create train/val/test splits (fixes applied)\n",
    "5. Label mappings\n",
    "6. Image processor & transforms\n",
    "7. Apply transforms to dataset (preprocessing)\n",
    "8. Data collator & dataloader test\n",
    "9. Model creation\n",
    "10. Training arguments\n",
    "11. Trainer and training\n",
    "12. Evaluation & reporting\n",
    "13. Save / export model\n",
    "\n",
    "**Practical notes:** reduce batch size if you run out of GPU memory. If running CPU-only, set `fp16=False` and small batch sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a3f37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate evaluate scikit-learn matplotlib torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db646060",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "from torchvision.transforms import (\n",
    "    Compose, Resize, CenterCrop, RandomResizedCrop, RandomHorizontalFlip, ToTensor, Normalize\n",
    ")\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8b0a7",
   "metadata": {},
   "source": [
    "Load the Food-101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855258ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\")\n",
    "print(ds)  # shows available splits (usually 'train' and 'test')\n",
    "print(\"Features:\", ds['train'].features)\n",
    "print(\"Num classes:\", len(ds['train'].features['label'].names))\n",
    "# show a small sample image and label to verify\n",
    "sample = ds['train'][0]\n",
    "print(\"Sample keys:\", sample.keys())\n",
    "print(\"Sample label id / name:\", sample['label'], ds['train'].features['label'].names[sample['label']])\n",
    "plt.imshow(sample['image'])\n",
    "plt.axis('off')\n",
    "plt.title(ds['train'].features['label'].names[sample['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51058d",
   "metadata": {},
   "source": [
    "Create train/validation split from the original 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436e25a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "val_fraction = 0.1\n",
    "split = ds['train'].train_test_split(test_size=val_fraction, seed=42)\n",
    "datasets = {\n",
    "    \"train\": split['train'],\n",
    "    \"val\": split['test'],\n",
    "    \"test\": ds['test']  # use official test split\n",
    "}\n",
    "print({k: len(v) for k, v in datasets.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4e63c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_names = datasets['train'].features['label'].names\n",
    "id2label = {i: name for i, name in enumerate(class_names)}\n",
    "label2id = {name: i for i, name in id2label.items()}\n",
    "print(\"Example labels:\", list(id2label.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c96fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f46535",
   "metadata": {},
   "source": [
    "processor.image_mean and image_std are lists; processor.size is dict or int depending on version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd193ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "img_size = processor.size[\"height\"] if isinstance(processor.size, dict) else processor.size\n",
    "mean, std = processor.image_mean, processor.image_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c4134",
   "metadata": {},
   "source": [
    "torchvision transforms (we will convert PIL -> tensor -> normalized using processor's mean/std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d28a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "val_test_transforms = Compose([\n",
    "    Resize((img_size, img_size)),\n",
    "    CenterCrop(img_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "print(\"Image size:\", img_size, \"mean/std:\", mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c631ef0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755428a",
   "metadata": {},
   "source": [
    "We'll convert each example image with the transforms and store 'pixel_values' as float tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ba2cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_transform_examples(example):\n",
    "    imgs = [train_transforms(Image.fromarray(img).convert(\"RGB\")) for img in example[\"image\"]]\n",
    "    # The dataset expects plain Python objects; Torch tensors are fine, but to be safe convert to numpy\n",
    "    example[\"pixel_values\"] = [img.numpy() for img in imgs]\n",
    "    return example\n",
    "\n",
    "def val_transform_examples(example):\n",
    "    imgs = [val_test_transforms(Image.fromarray(img).convert(\"RGB\")) for img in example[\"image\"]]\n",
    "    example[\"pixel_values\"] = [img.numpy() for img in imgs]\n",
    "    return example\n",
    "\n",
    "# Set transforms. These are applied lazily on access.\n",
    "datasets[\"train\"] = datasets[\"train\"].with_transform(train_transform_examples)\n",
    "datasets[\"val\"] = datasets[\"val\"].with_transform(val_transform_examples)\n",
    "datasets[\"test\"] = datasets[\"test\"].with_transform(val_transform_examples)\n",
    "\n",
    "# Quick check on one item\n",
    "item = datasets[\"train\"][0]\n",
    "print(\"pixel_values length:\", len(item[\"pixel_values\"]), \"label:\", item[\"label\"])\n",
    "import torch\n",
    "print(\"pixel_values shape example (converted to tensor):\", torch.tensor(item[\"pixel_values\"][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8d5de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate_fn(batch):\n",
    "    pixel_vals = torch.stack([torch.tensor(x[\"pixel_values\"][0]) if isinstance(x[\"pixel_values\"], list) else torch.tensor(x[\"pixel_values\"]) for x in batch])\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    return {\"pixel_values\": pixel_vals, \"labels\": labels}\n",
    "\n",
    "# Test DataLoader (small batch)\n",
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(datasets[\"train\"], batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "batch = next(iter(train_dl))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"pixel_values shape:\", batch[\"pixel_values\"].shape)\n",
    "print(\"labels shape:\", batch[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b07e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01646486",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = \"food101-vit-model\"\n",
    "# if no GPU, set fp16=False and reduce batch sizes\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=8 if torch.cuda.is_available() else 2,\n",
    "    per_device_eval_batch_size=16 if torch.cuda.is_available() else 4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=1000,\n",
    "    fp16=torch.cuda.is_available(),  # only enable if CUDA available\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=4,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "print(train_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecd855",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p_r_f_s = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": p_r_f_s[0],\n",
    "        \"recall_macro\": p_r_f_s[1],\n",
    "        \"f1_macro\": p_r_f_s[2]\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"val\"],\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,   # ViTImageProcessor works here as tokenizer/feature extractor\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Trainer created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02fd6e",
   "metadata": {},
   "source": [
    "\n",
    "For quick testing, set num_train_epochs=1 and/or use a small subset of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c6e5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e377e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "small_train = datasets[\"train\"].select(range(256))\n",
    "small_val = datasets[\"val\"].select(range(128))\n",
    "trainer_small = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_val,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_small.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c368c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "preds_output = trainer.predict(datasets[\"test\"])\n",
    "print(\"Test metrics (HF):\", preds_output.metrics)\n",
    "\n",
    "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Overall accuracy:\", (y_pred == y_true).mean())\n",
    "print(\"\\nClassification report (first 10 classes):\")\n",
    "print(classification_report(y_true, y_pred, labels=list(range(10)), target_names=class_names[:10], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc09c58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved model and processor to\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
